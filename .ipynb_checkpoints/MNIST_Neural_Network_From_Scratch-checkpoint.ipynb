{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32b4edd0-7dc8-4598-b2e8-6d1968ad5ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Packages Standard Stuff\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4c85bc5-8344-486e-9007-ee08869b7eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull in our Training Data\n",
    "data = pd.read_csv('train.csv')\n",
    "#We need to use numpy to make efficent and fast Lin Alg calculations\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "247ef0ab-351a-4522-ac7f-bfbfcd4a79cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Dimensions of Data\n",
    "m,n = data.shape\n",
    "np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "434628d8-ceb7-4595-ab8b-889bebe23ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can now split our data into two categories: dev and train\n",
    "# Dev will be used to ensure the model generalizes well by providing a check against overfitting and aiding in hyperparameter tuning.\n",
    "# Train will be used to train our model \n",
    "data_dev = data[0:1000].T\n",
    "Y_dev = data_dev[0]\n",
    "X_dev = data_dev[1:n]\n",
    "\n",
    "data_train = data[1000:m].T\n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fe16c22-a638-471d-a8cf-57a5841db0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are starting our weights and biases with random values, from which we will then train later\n",
    "def init_params():\n",
    "    W1 = np.random.randn(10, 784)\n",
    "    b1 = np.random.randn(10, 1)\n",
    "    W2 = np.random.randn(10, 784)\n",
    "    b2 = np.random.randn(10, 1)\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "#We are defining our non-linear function RElU => x>0 -> x but if x<=0 -> 0 \n",
    "def ReLU(value):\n",
    "    if value > 0:\n",
    "        return value\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "#Softmax function essentially gets us the probability for the image being any of 10 numbers possible in this dataset\n",
    "def softmax(Z):\n",
    "    return exp(Z) / np.sum(exp(Z))\n",
    "\n",
    "def forward_prop(W1, b1, W2, b2, X):\n",
    "     Z1 = W1.dot(X) + b1\n",
    "     A1 = ReLU(Z1)\n",
    "     Z2 = W2.dot(A1) + b2\n",
    "     A2 = softmax(Z2)\n",
    "     return Z1, A1, Z2, A2\n",
    "\n",
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y\n",
    "\n",
    "def ReLU_deriv(Z):\n",
    "    return Z > 0\n",
    "\n",
    "#Use Standard Backpropgation and the calulcus mess that is required\n",
    "def back_prop(Z1, A1, Z2, A2, Y):\n",
    "    one_hot_Y = one_hot(Y)\n",
    "    dZ2 = A2 - one_hot_Y\n",
    "    dW2 = 1 / m * dZ2.dot(A1.T)\n",
    "    db2 = 1 / m * np.sum(dZ2)\n",
    "    dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)\n",
    "    dW1 = 1 / m * dZ1.dot(X.T)\n",
    "    db1 = 1 / m * np.sum(dZ1)\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "#Update params for steepest loss function decrease\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * db1    \n",
    "    W2 = W2 - alpha * dW2  \n",
    "    b2 = b2 - alpha * db2    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2eb9629-e128-4bf1-8b1d-ac250cb231de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    print(predictions, Y)\n",
    "    return np.sum(predictions == Y) / Y.size\n",
    "\n",
    "def gradient_descent(X, Y, alpha, iterations):\n",
    "    W1, b1, W2, b2 = init_params()\n",
    "    for i in range(iterations):\n",
    "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "        dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "        if i % 10 == 0:\n",
    "            print(\"Iteration: \", i)\n",
    "            predictions = get_predictions(A2)\n",
    "            print(get_accuracy(predictions, Y))\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aff4081b-ef23-4078-a6d7-5ef49879a865",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m W1, b1, W2, b2 \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 11\u001b[0m, in \u001b[0;36mgradient_descent\u001b[0;34m(X, Y, alpha, iterations)\u001b[0m\n\u001b[1;32m      9\u001b[0m W1, b1, W2, b2 \u001b[38;5;241m=\u001b[39m init_params()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iterations):\n\u001b[0;32m---> 11\u001b[0m     Z1, A1, Z2, A2 \u001b[38;5;241m=\u001b[39m \u001b[43mforward_prop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     dW1, db1, dW2, db2 \u001b[38;5;241m=\u001b[39m backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n\u001b[1;32m     13\u001b[0m     W1, b1, W2, b2 \u001b[38;5;241m=\u001b[39m update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
      "Cell \u001b[0;32mIn[15], line 22\u001b[0m, in \u001b[0;36mforward_prop\u001b[0;34m(W1, b1, W2, b2, X)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_prop\u001b[39m(W1, b1, W2, b2, X):\n\u001b[1;32m     21\u001b[0m      Z1 \u001b[38;5;241m=\u001b[39m W1\u001b[38;5;241m.\u001b[39mdot(X) \u001b[38;5;241m+\u001b[39m b1\n\u001b[0;32m---> 22\u001b[0m      A1 \u001b[38;5;241m=\u001b[39m \u001b[43mReLU\u001b[49m\u001b[43m(\u001b[49m\u001b[43mZ1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m      Z2 \u001b[38;5;241m=\u001b[39m W2\u001b[38;5;241m.\u001b[39mdot(A1) \u001b[38;5;241m+\u001b[39m b2\n\u001b[1;32m     24\u001b[0m      A2 \u001b[38;5;241m=\u001b[39m softmax(Z2)\n",
      "Cell \u001b[0;32mIn[15], line 11\u001b[0m, in \u001b[0;36mReLU\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mReLU\u001b[39m(value):\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 0.10, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e747f7-a86f-47ac-8f90-a6e76259ed7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
